{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOaQU3U5OUNTafQZYcI3MqW"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Q1. What is an activation function in the context of artificial neural networks?"
      ],
      "metadata": {
        "id": "_jWbYjiotd4H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An activation function is a mathematical operation applied to each node (or neuron) in a neural network, specifically in the context of artificial neural networks. It introduces non-linearity to the network, enabling it to learn complex patterns in data. The activation function determines whether a neuron should be activated (output a signal) or not, based on the weighted sum of its inputs.\n",
        "\n",
        "In a neural network, information flows through interconnected nodes, and at each node, the input values are multiplied by weights, summed up, and then passed through an activation function. This activation function helps introduce non-linearities to the model, allowing it to learn and represent more complex relationships in the data."
      ],
      "metadata": {
        "id": "K3GZuWr3t7fU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2. What are some common types of activation functions used in neural networks?"
      ],
      "metadata": {
        "id": "ItXatN4_vSsW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Sigmoid Activation Function it squashes the output between 0 and 1 , its often used in Binary Classification\n",
        "\n",
        "\n",
        "- Hyperbolic Tangent (tanh) -> It squashes the value between -1 and 1 . tanH have much steeper curve than sigmoid Activation Function , Area Under the curve is also large than sigmoid activation function , and it tanH curve is o centric\n",
        "\n",
        "\n",
        "- Softmax Activation Function -> It is basically used for multiclass Problem\n",
        "\n",
        "- ReLu Activation Fuction - It introduces non-linearity to the model by outputting the input directly if it is positive, and zero otherwise.It has an issue of Dead weight or Dying Relu\n",
        "\n",
        "- Leaky ReLu- It is special case of parametric ReLU in Leaky ReLu we solved dead neuron problem here value ranges from 0.001 ,0.1 ,If alpha value other than 0.01 than it is Randomised ReLu. If alpha is learnable than it is parametric Relu and alpha is fixed\n",
        "â€‹\n"
      ],
      "metadata": {
        "id": "7Vi1gDsPvvJu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q3. How do activation functions affect the training process and performance of a neural network?"
      ],
      "metadata": {
        "id": "zMllzLVG3w4V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Activation functions introduce non-linearity to neural networks, allowing them to learn complex patterns and relationships in the data.\n",
        "\n",
        "-  Activation functions impact the flow of gradients during backpropagation, influencing how weights are updated during training.\n",
        "\n",
        "- Activation functions affect the stability and convergence speed of the training process, with choices like ReLU often leading to faster convergence."
      ],
      "metadata": {
        "id": "SsQd5ntz36-c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####-Q4. How does the sigmoid activation function work? What are its advantages and disadvantages?"
      ],
      "metadata": {
        "id": "akGmpf-o65rN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sigmoid activation function squashes the range between pobability value 0 and 1 , basically used for Binary Classification\n",
        "\n",
        "#### Advantages\n",
        "\n",
        " - the sigmoid function has a smooth derivative, making it well-suited for gradient-based optimization algorithms like gradient descent.\n",
        "\n",
        "#### Disvantages\n",
        "- leading to vanishing gradient problem during backpropagation.This can make learning rate slow\n",
        "- The output of the sigmoid function is not centered around zero, which can lead to convergence issues\n",
        "- Basically used only for binary Classification\n"
      ],
      "metadata": {
        "id": "CBeSID-M8Yg8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q5.What is the rectified linear unit (ReLU) activation function? How does it differ from the sigmoid function?"
      ],
      "metadata": {
        "id": "O9VfhEtX_Ngd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- It introduces non-linearity to the model by outputting the input directly if it is positive, and zero otherwise.It has an issue of Dead weight or Dying Relu\n",
        "- ReLU(x) = max(x,0)\n",
        "    - if input is positive it gives a positive output\n",
        "    - if input is negative it gives a output as zero\n",
        "\n",
        "###### Difference from Sigmoid Function:\n",
        "  - Sigmoid: Produces outputs in the range (0, 1), interpreting them as probabilities.\n",
        "  - ReLU: Outputs non-negative values, ranging from 0 to positive infinity\n",
        "  - Sigmoid Prone to the vanishing gradient problem, particularly in deep networks, where gradients can become very small during backpropagation.\n",
        "  -ReLU Helps mitigate the vanishing gradient problem, as it does not saturate for positive inputs, allowing gradients to flow more easily during backpropagation.\n",
        "  - Sigmoid is Computationally more Expensive because of exponential Function ReLU is computaionally efficient"
      ],
      "metadata": {
        "id": "6Khf16r3_cVk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q6. What are the benefits of using the ReLU activation function over the sigmoid function?\n"
      ],
      "metadata": {
        "id": "qH6ioEszC0XE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Difference from Sigmoid Function:\n",
        "- Sigmoid: Produces outputs in the range (0, 1), interpreting them as probabilities.\n",
        "- ReLU: Outputs non-negative values, ranging from 0 to positive infinity\n",
        "- Sigmoid Prone to the vanishing gradient problem, particularly in deep networks, where gradients can become very small during backpropagation.\n",
        "- ReLU Helps mitigate the vanishing gradient problem, as it does not saturate for positive inputs, allowing gradients to flow more easily during backpropagation.\n",
        "- Sigmoid is Computationally more Expensive because of exponential Function ReLU is computaionally efficient"
      ],
      "metadata": {
        "id": "Jr89iCGFC3u7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q7. Explain the concept of \"leaky ReLU\" and how it addresses the vanishing gradient problem."
      ],
      "metadata": {
        "id": "L_NEwusmDIw7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Leaky Relu is a variant of Relu Activation fuction which solve the problem of dead Relu when input is negative Leaky ReLU introduces a small, non-zero slope for negative inputs, allowing a small, positive gradient to flow through during backpropagation\n",
        "\n",
        "- vanishing gradient problem when the gradient during backpropagation becomes very small or closer to zero Standard ReLU can suffer from the \"dying ReLU\" problem, where neurons become inactive (output zero) for all inputs during training\n",
        "- Leaky ReLU helps address the vanishing gradient problem by allowing a small, non-zero gradient for negative inputs. This ensures that even if a neuron is not active for positive inputs, it can still convey information for negative inputs, preventing the neuron from becoming entirely inactive.\n",
        "- The small slope for negative inputs helps maintain a gradient flow through the network, enabling more effective learning, especially in deeper architectures. Leaky ReLU strikes a balance between introducing non-linearity and preventing the issues associated with dead neurons in the standard ReLU,"
      ],
      "metadata": {
        "id": "GjZwtxpyDo0D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q8. What is the purpose of the softmax activation function? When is it commonly used?"
      ],
      "metadata": {
        "id": "n68KqxJ_Fw5z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The softmax activation function is commonly used in the output layer of a neural network, especially in multi-class classification tasks it is similar to sigmoid activation function but used for multi-class classification\n",
        "\n",
        "- Softmax ensures that the output values lie in the range (0, 1) and sum to 1.0, forming a valid probability distribution.\n",
        "The output of the softmax function can be interpreted as the probability of each class, making it suitable for multi-class classification problems.\n",
        "\n",
        "\n",
        "- Softmax is commonly used in the output layer of neural networks for tasks where an input can belong to one of multiple classes.\n",
        "Examples include image classification, natural language processing tasks like sentiment analysis or part-of-speech tagging, and any scenario where the input can be categorized into one of several distinct classes."
      ],
      "metadata": {
        "id": "u40u9nzbGINb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q9. What is the hyperbolic tangent (tanh) activation function? How does it compare to the sigmoid function?"
      ],
      "metadata": {
        "id": "4JmTB-opG2NK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- It squashes the value between -1 and 1 .  sigmoid Activation Function Ranges from 0 to 1\n",
        "-  tanH have much steeper curve than sigmoid Activation Function , Area Under the curve is also large than sigmoid activation function\n",
        "-  Outputs are not centerd  around zero which may lead to convergence issue , TanH curve is zero centric  curve so there is no convergence issue\n",
        "- Both sigmoid and tanh functions can suffer from the vanishing gradient problem, especially for extreme input values.\n",
        "- Sigmoid is often used in the output layer for binary classification tasks.\n",
        "\n",
        "- Tanh is commonly used in the hidden layers of neural networks, especially when zero-centered outputs are desired"
      ],
      "metadata": {
        "id": "_zUdEUCmHEKi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ew6VL2YXtbnI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}